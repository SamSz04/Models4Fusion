import os
import dataclasses
from functools import partial

import jax
import jax.numpy as jnp
from flax import nnx
from flax.linen.pooling import max_pool


# ==========================================
# 1. è®¾ç½® XLA Dump ç¯å¢ƒå˜é‡
# ==========================================
# è¿™å‘Šè¯‰ç¼–è¯‘å™¨æŠŠä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰ä¿å­˜åˆ°å“ªé‡Œ
# --xla_dump_hlo_as_text: ä¿å­˜ä¸º .txt æ–‡ä»¶ï¼Œæ–¹ä¾¿é˜…è¯»
# --xla_dump_hlo_as_proto: ä¿å­˜ä¸º .pb æ–‡ä»¶ï¼Œæ–¹ä¾¿å·¥å…·è§£æ
os.environ["XLA_FLAGS"] = (
    "--xla_dump_to=./xla_dumps_resnet "
    "--xla_dump_hlo_as_text "
    "--xla_dump_hlo_as_dot"
)


@dataclasses.dataclass(frozen=True)
class ModelConfig:
    block_layers: list[int]
    num_classes: int

    def resnet50(num_classes: int = 1000):
        return ModelConfig([3, 4, 6, 3], num_classes=num_classes)

    def resnet152(num_classes: int = 1000):
        return ModelConfig([3, 8, 36, 3], num_classes=num_classes)


class Bottleneck(nnx.Module):
    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, downsample=None, *, rngs: nnx.Rngs):
        self.conv0 = nnx.Conv(
            in_channels, out_channels, kernel_size=(1, 1), strides=1, padding=0, use_bias=False, rngs=rngs
        )
        self.bn0 = nnx.BatchNorm(out_channels, use_running_average=True, rngs=rngs)

        self.conv1 = nnx.Conv(
            out_channels, out_channels, kernel_size=(3, 3), strides=stride, padding=1, use_bias=False, rngs=rngs
        )
        self.bn1 = nnx.BatchNorm(out_channels, use_running_average=True, rngs=rngs)

        self.conv2 = nnx.Conv(
            out_channels, out_channels * 4, kernel_size=(1, 1), strides=1, padding=0, use_bias=False, rngs=rngs
        )
        self.bn2 = nnx.BatchNorm(out_channels * 4, use_running_average=True, rngs=rngs)

        self.downsample = downsample

    def __call__(self, x):
        identity = x

        x = self.conv0(x)
        x = self.bn0(x)
        x = nnx.relu(x)

        x = self.conv1(x)
        x = self.bn1(x)
        x = nnx.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)

        if self.downsample is not None:
            identity = self.downsample(identity)

        return nnx.relu(x + identity)


class Downsample(nnx.Module):
    def __init__(self, in_channels: int, out_channels: int, stride: int, *, rngs: nnx.Rngs):
        self.conv = nnx.Conv(
            in_channels, out_channels, kernel_size=(1, 1), strides=stride, padding=0, use_bias=False, rngs=rngs
        )
        self.bn = nnx.BatchNorm(out_channels, use_running_average=True, rngs=rngs)

    def __call__(self, x):
        x = self.conv(x)
        return self.bn(x)


class BlockGroup(nnx.Module):
    def __init__(self, in_channels: int, out_channels: int, blocks, stride: int, *, rngs: nnx.Rngs):
        self.blocks = nnx.List()

        downsample = None
        if stride != 1 or in_channels != out_channels * 4:
            downsample = Downsample(in_channels, out_channels * 4, stride, rngs=rngs)

        self.blocks.append(Bottleneck(in_channels, out_channels, stride, downsample, rngs=rngs))
        for _ in range(1, blocks):
            self.blocks.append(Bottleneck(out_channels * 4, out_channels, stride=1, downsample=None, rngs=rngs))

    def __call__(self, x):
        for block in self.blocks:
            x = block(x)
        return x


class Stem(nnx.Module):
    def __init__(self, *, rngs: nnx.Rngs):
        self.conv = nnx.Conv(3, 64, kernel_size=(7, 7), strides=2, padding=3, use_bias=False, rngs=rngs)
        self.bn = nnx.BatchNorm(64, use_running_average=True, rngs=rngs)
        self.pool = partial(max_pool, window_shape=(3, 3), strides=(2, 2), padding=((1, 1), (1, 1)))

    def __call__(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = nnx.relu(x)
        x = self.pool(x)
        return x


class ResNet(nnx.Module):
    def __init__(self, cfg: ModelConfig, *, rngs: nnx.Rngs):
        self.stem = Stem(rngs=rngs)

        self.layer0 = BlockGroup(64, 64, cfg.block_layers[0], stride=1, rngs=rngs)
        self.layer1 = BlockGroup(256, 128, cfg.block_layers[1], stride=2, rngs=rngs)
        self.layer2 = BlockGroup(512, 256, cfg.block_layers[2], stride=2, rngs=rngs)
        self.layer3 = BlockGroup(1024, 512, cfg.block_layers[3], stride=2, rngs=rngs)

        self.pool = partial(lambda x: x.mean(axis=(1, 2)))
        self.fc = nnx.Linear(2048, cfg.num_classes, rngs=rngs)

    def __call__(self, x):
        x = self.stem(x)
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.pool(x)
        return self.fc(x)


@jax.jit
def forward(model, x):
    return model(x)


# ==========================================
# 2. æ‰§è¡Œé€»è¾‘ï¼šåˆå§‹åŒ–ä¸è§¦å‘ç¼–è¯‘
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– ResNet50 (NNX)...")

    # 1. åˆå§‹åŒ–æ¨¡å‹
    # NNX çš„ç‰¹ç‚¹æ˜¯æ¨¡å‹å¯¹è±¡æœ¬èº«æŒæœ‰å‚æ•° (Stateful)
    rngs = nnx.Rngs(0)  # è®¾ç½®éšæœºç§å­
    config = ModelConfig.resnet50()
    model = ResNet(config, rngs=rngs)

    # 2. æ„å»º Dummy Input (ä¼ªé€ è¾“å…¥)
    # ResNet æ ‡å‡†è¾“å…¥é€šå¸¸æ˜¯ (Batch, Height, Width, Channels)
    # è¿™é‡Œä½¿ç”¨ Batch=1 æ¥è·å–å•æ¬¡æ¨ç†çš„å›¾
    input_shape = (1, 224, 224, 3)
    x = jnp.ones(input_shape, dtype=jnp.float32)


    # 3. å®šä¹‰ JIT ç¼–è¯‘çš„æ¨ç†å‡½æ•°
    # NNX æ¨¡å‹å¯ä»¥ç›´æ¥ä½œä¸ºå‚æ•°ä¼ é€’ç»™ JIT å‡½æ•°ï¼ŒJAX ä¼šè‡ªåŠ¨å¤„ç†å…¶ PyTree ç»“æ„
    @jax.jit
    def inference_step(model, x):
        # æ³¨æ„ï¼šä½ çš„ BatchNorm è®¾ç½®äº† use_running_average=True
        # è¿™æ„å‘³ç€è¿™æ˜¯çº¯æ¨ç†æ¨¡å¼ï¼Œä¸ä¼šæ›´æ–° Batch Statsï¼Œéå¸¸é€‚åˆ Dump é™æ€å›¾
        return model(x)


    print("âš¡ï¸ å¼€å§‹ JIT ç¼–è¯‘å¹¶è§¦å‘ XLA Dump...")
    print(f"   è¾“å…¥å½¢çŠ¶: {input_shape}")

    # 4. è¿è¡Œä¸€æ¬¡ä»¥è§¦å‘ Tracing å’Œç¼–è¯‘
    # è¿™ä¸€æ­¥å®Œæˆåï¼Œ./xla_dumps_resnet æ–‡ä»¶å¤¹ä¸‹å°±ä¼šç”Ÿæˆ HLO æ–‡ä»¶
    logits = inference_step(model, x)

    print(f"âœ… å®Œæˆï¼è¾“å‡º Logits å½¢çŠ¶: {logits.shape}")
    print("ğŸ“ è¯·æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„ 'xla_dumps_resnet' æ–‡ä»¶å¤¹è·å– HLO æ–‡ä»¶ã€‚")
    print("   é‡ç‚¹å¯»æ‰¾åŒ…å« 'before_optimizations' å’Œ 'ir_with_opt' çš„ txt æ–‡ä»¶ã€‚")